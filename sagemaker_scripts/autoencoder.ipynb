{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoder Example: Distributed training and Hyperparameter Tuning with Amazon SageMaker\n",
    "\n",
    "Training the spatio-temporal stacked frame AutoEncoder is significatnly slower. For this reason We will use SageMaker in this notebook to optimize the model parameters and to train the model in parallel on multiple hosts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "print (role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Deploy the standard way\n",
    "You need to upload the training data to an S3 bucket: check the script ```upload_data.py``` how to upload data to S3.\n",
    "Once the data is uploaded you can define the MXNet Estimator, which takes as argument an entry point ```train.py```, the role, the training instance type, the path where data is located and another path where the code shall be uploaded. If you don't indicate these paths, SageMaker will use a default bucket. Next we have to define the Deep Learning framework we want to use and the hyperparameters.\n",
    "\n",
    "It can be useful for debugging purposes to define the instance type as local in the beginning. Then the SageMaker will execute your code in your local Notebook instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.mxnet import MXNet\n",
    "\n",
    "MY_S3_BUCKET = ''\n",
    "\n",
    "mxnet_estimator = MXNet('train.py',\n",
    "                        role=role,\n",
    "                        train_instance_type='local',#'ml.m5.xlarge',\n",
    "                        train_instance_count=1,\n",
    "                        output_path='s3://MY_S3_BUCKET',\n",
    "                        code_location='s3://MY_S3_BUCKET',\n",
    "                        framework_version='1.3.0', py_version='py2',\n",
    "                        hyperparameters={'batch_size': 16,\n",
    "                         'epochs': 10,\n",
    "                         'learning_rate': 0.0001,\n",
    "                         'wd': 0.0})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call fit on our training data. Behind the scenes SageMaker spin up your EC2 instance indicated in ```train_instance_type``` (if not set to local). Once the instance is ready SageMaker will download a MXNet Docker container, and execute the function ```train()``` from ```train.py```, which creates and trains the model. After training the model is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mxnet_estimator.fit({'train': 's3://MY_S3_BUCKET/data/input_data.npy'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our autoencoder model is trained we can deploy it. Here we define that the endpoint shall run on a ```m5.xlarge``` instance, which does not provide any GPUs. Inference won't run very fast, but this instance type is therefore cheaper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = mxnet_estimator.deploy(instance_type='ml.m5.xlarge', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the endpoint is ready, we can send requests to it. SageMaker provides standard code for model inference. But often it is useful to customize these functions, for this reason ```train.py``` overwrites the default ```model_fn```. In the following example we send a numpy array filled with zeros to the endpoint. The endpoint will verify the user request, parse the input, then load the model and return the inference results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import numpy_deserializer, npy_serializer\n",
    "import numpy as np\n",
    "\n",
    "#predictor.accept = 'application/x-npy'\n",
    "#predictor.content_type = 'application/x-npy'\n",
    "#predictor.deserializer =  numpy_deserializer\n",
    "#predictor.serializer =  npy_serializer\n",
    "print(predictor.predict(np.zeros((10,10,227,277))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Hyperparameter Tuning Job\n",
    "Define HyperparameterTuner, which takes our MXNetEstimator, some hyperparameters and the metric that shall be optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner, ContinuousParameter, IntegerParameter \n",
    "\n",
    "tuner = HyperparameterTuner(estimator=mxnet_estimator,  \n",
    "                               objective_metric_name='loss',\n",
    "                               hyperparameter_ranges={'learning_rate': ContinuousParameter(0.00001, 0.0001), \n",
    "                                                      'epochs': IntegerParameter(5,50),\n",
    "                                                      'wd': ContinuousParameter(0, 0.001) },\n",
    "                               metric_definitions=[{'Name': 'loss', 'Regex': 'loss:([0-9\\\\.]+)'}],\n",
    "                               max_jobs=40,\n",
    "                               max_parallel_jobs=5,\n",
    "                               objective_type='Minimize')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the hyperparameter tuning jobs. This will create in total 40 tuning jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit({'train': 's3://MY_S3_BUCKET/data/input_data.npy'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the jobs are ready we can pick the best one and deploy this as an endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.deploy(instance_type='ml.m5.xlarge', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Training\n",
    "The goal is to run a data parallel jobs, where each host runs the same model training but on different data. \n",
    "Instead of using ```train.py``` as entry point, we use a modified version ```train_distributed.py```. The new script takes care of reading in the right input file, setting up the ```kvstore``` and gathering, merging the overall losses. We have to increase ```train_instance_count```, otherwise we won't run on multiple hosts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxnet_estimator = MXNet('train_distributed.py',\n",
    "                        role=role,\n",
    "                        train_instance_type='ml.m5.xlarge',\n",
    "                        train_instance_count=2, \n",
    "                        output_path='s3://MY_S3_BUCKET',\n",
    "                        code_location='s3://MY_S3_BUCKET',\n",
    "                        framework_version='1.3.0', py_version='py2',\n",
    "                        hyperparameters={'batch_size': 16,\n",
    "                         'epochs': 10,\n",
    "                         'learning_rate': 0.0001,\n",
    "                         'wd': 0.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxnet_estimator.fit('s3://MY_S3_BUCKET/data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
